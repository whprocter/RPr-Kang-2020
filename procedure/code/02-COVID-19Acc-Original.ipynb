{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: RP- Spatial Accessibility of COVID-19 Healthcare Resources in Illinois\n",
    "---\n",
    "\n",
    "### Reproduction\n",
    "\n",
    "**Reproduction of**: Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA\n",
    "\n",
    "Original study *by* Kang, J. Y., A. Michels, F. Lyu, Shaohua Wang, N. Agbodo, V. L. Freeman, and Shaowen Wang. 2020. Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA. International Journal of Health Geographics 19 (1):1â€“17. DOI:[10.1186/s12942-020-00229-x](https://ij-healthgeographics.biomedcentral.com/articles/10.1186/s12942-020-00229-x).\n",
    "\n",
    "Reproduction Authors: Joe Holler, Kufre Udoh, Derrick Burt, Drew An-Pham, & Spring '21 Middlebury Geog 0323.\n",
    "\n",
    "Reproduction Materials Available at: [github.com/HEGSRR/RPr-Kang-2020](https://github.com/HEGSRR/RPr-Kang-2020)\n",
    "\n",
    "Created: `8 Jun 2021`\n",
    "Revised: `26 Feb 2023`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Environment\n",
    "The original study was conducted using an undocumented version of Python on the University of Illinois Urbana Champaign CyberGISX server.\n",
    "The current version of the study is designed to run on the CyberGISX Python 3 environment using Python version 3.8.12.\n",
    "See `environment.yml` for more details on the library versions.\n",
    "Key packages include the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# report python version and install required packages\n",
    "# switch if statement from False to True to install packages with Conda\n",
    "if False:\n",
    "    !python -V\n",
    "    !conda install -c conda-forge osmnx=1.1.2 -y\n",
    "    !conda env update --file ../environment/environment.yml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import re\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import folium\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set working directory\n",
    "\n",
    "Set the working directory to the root of the research compendium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if the working directory is the 'code' folder, move up two folders to the compendium root\n",
    "if os.path.basename(os.getcwd()) == 'code':\n",
    "    # This line moves the directory up two folders \n",
    "    os.chdir('../../')\n",
    "print('The working directory is', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "To perform the ESFCA method, three types of data are required, as follows: (1) road network, (2) population, and (3) hospital information. The road network can be obtained from the [OpenStreetMap Python Library, called OSMNX](https://github.com/gboeing/osmnx). The population data is available on the [American Community Survey](https://data.census.gov/cedsci/deeplinks?url=https%3A%2F%2Ffactfinder.census.gov%2F&tid=GOVSTIMESERIES.CG00ORG01). Lastly, hosptial information is also publically available on the [Homelanad Infrastructure Foundation-Level Data](https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals?geometry=-94.504%2C40.632%2C-80.980%2C43.486)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population and COVID-19 Cases Data by County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for at risk population\n",
    "atrisk_data = gpd.read_file('./data/raw/public/PopData/Chicago_Tract.shp')\n",
    "atrisk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for covid cases\n",
    "covid_data = gpd.read_file('./data/raw/public/PopData/Chicago_ZIPCODE.shp')\n",
    "covid_data['cases'] = covid_data['cases']\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hospital Data\n",
    "\n",
    "Note that 999 is treated as a \"NULL\"/\"NA\" so these hospitals are filtered out. This data contains the number of ICU beds and ventilators at each hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data for hospitals\n",
    "hospitals = gpd.read_file('./data/raw/public/HospitalData/Chicago_Hospital_Info.shp')\n",
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Plot Map of Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot hospitals\n",
    "m = folium.Map(location=[41.85, -87.65], tiles='cartodbpositron', zoom_start=10)\n",
    "\n",
    "for i in range(0, len(hospitals)):\n",
    "    folium.CircleMarker(\n",
    "      location=[hospitals.iloc[i]['Y'], hospitals.iloc[i]['X']],\n",
    "      popup=\"{}{}\\n{}{}\\n{}{}\".format('Hospital Name: ',hospitals.iloc[i]['Hospital'],\n",
    "                                      'ICU Beds: ',hospitals.iloc[i]['Adult ICU'],\n",
    "                                      'Ventilators: ', hospitals.iloc[i]['Total Vent']),\n",
    "      radius=5,\n",
    "      color='grey',\n",
    "      fill=True,\n",
    "      fill_opacity=0.6,\n",
    "      legend_name = 'Hospitals'\n",
    "    ).add_to(m)\n",
    "legend_html =   '''<div style=\"position: fixed; width: 20%; heigh: auto;\n",
    "                            bottom: 10px; left: 10px;\n",
    "                            solid grey; z-index:9999; font-size:14px;\n",
    "                            \">&nbsp; Legend<br>'''\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Plot Hexagon Grid (500-meter resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load grid file and plot\n",
    "grid_file = gpd.read_file('./data/raw/public/GridFile/Chicago_Grid.shp')\n",
    "grid_file = grid_file.drop(columns=[\"left\",\"right\",\"top\",\"bottom\"])\n",
    "grid_file.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load Street Network\n",
    "\n",
    "Load a street network for either the extent of Chicago, Illinois (provided with the original research compendium) or for the extent of Chicago, Illinois expanded with a buffer distance of 15 miles (24140.2 meters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "buffered = True\n",
    "# change buffered to True to load an expanded (buffered) street network\n",
    "\n",
    "if not buffered:\n",
    "    print(\"Loading original unbuffered Chicago road network from data/raw/public/Chicago_Network.graphml. Please wait...\", flush=True)\n",
    "    G = ox.load_graphml('./data/raw/public/Chicago_Network.graphml') \n",
    "\n",
    "# add code to download from OSF @ https://osf.io/download/z8ery/ \n",
    "elif not os.path.exists(\"./data/raw/private/Chicago_Network_Buffer.graphml\"):\n",
    "    print(\"Loading buffered Chicago road network from OpenStreetMap. Please wait... runtime may exceed 9min...\", flush=True)\n",
    "    G = ox.graph_from_place('Chicago', network_type='drive', buffer_dist=24140.2) \n",
    "    print(\"Saving Chicago road network to raw/private/Chicago_Network_Buffer.graphml. Please wait...\", flush=True)\n",
    "    ox.save_graphml(G, './data/raw/private/Chicago_Network_Buffer.graphml')\n",
    "    print(\"Data saved.\")\n",
    "\n",
    "else:\n",
    "    print(\"Loading buffered Chicago road network from raw/private/Chicago_Network_Buffer.graphml. Please wait...\", flush=True)\n",
    "    G = ox.load_graphml('./data/raw/private/Chicago_Network_Buffer.graphml') \n",
    "    print(\"Data loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Road Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "ox.plot_graph(G, node_size = 1, bgcolor = 'white', node_color = 'black', edge_color = \"#333333\", node_alpha = 0.5, edge_linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "In this section, raw data is processed in preparation for the accessibility analysis. Functions are first defined and then called to process the data.\n",
    "\n",
    "### network_setting\n",
    "\n",
    "Cleans the OSMNX network to work better with drive-time analysis.\n",
    "\n",
    "First, we remove all nodes with 0 outdegree because any hospital assigned to such a node would be unreachable from everywhere. Next, we remove small (under 10 node) *strongly connected components* to reduce erroneously small ego-centric networks. Lastly, we ensure that the max speed is set and in the correct units before calculating time.\n",
    "\n",
    "Args:\n",
    "\n",
    "* network: OSMNX network for the spatial extent of interest\n",
    "\n",
    "Returns:\n",
    "\n",
    "* OSMNX network: cleaned OSMNX network for the spatial extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def network_setting(network):\n",
    "    _nodes_removed = len([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    network.remove_nodes_from([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    for component in list(nx.strongly_connected_components(network)):\n",
    "        if len(component)<10:\n",
    "            for node in component:\n",
    "                _nodes_removed+=1\n",
    "                network.remove_node(node)\n",
    "    for u, v, k, data in tqdm(G.edges(data=True, keys=True),position=0):\n",
    "        if 'maxspeed' in data.keys():\n",
    "            speed_type = type(data['maxspeed'])\n",
    "            if (speed_type==str):\n",
    "                # Add in try/except blocks to catch maxspeed formats that don't fit Kang et al's cases\n",
    "                try:\n",
    "                    if len(data['maxspeed'].split(','))==2:\n",
    "                        data['maxspeed_fix']=float(data['maxspeed'].split(',')[0])                  \n",
    "                    elif data['maxspeed']=='signals':\n",
    "                        data['maxspeed_fix']=35.0 # Drive speed setting as 35 miles\n",
    "                    else:\n",
    "                        data['maxspeed_fix']=float(data['maxspeed'].split()[0])\n",
    "                except:\n",
    "                    data['maxspeed_fix']=35.0 # Miles\n",
    "            else:\n",
    "                try:\n",
    "                    data['maxspeed_fix']=float(data['maxspeed'][0].split()[0])\n",
    "                except:\n",
    "                    data['maxspeed_fix']=35.0 # Miles\n",
    "        else:\n",
    "            data['maxspeed_fix']= 35.0 # Miles\n",
    "        data['maxspeed_meters'] = data['maxspeed_fix']*26.8223 # Convert mile to meter\n",
    "        data['time'] = float(data['length'])/ data['maxspeed_meters']\n",
    "    print(\"Removed {} nodes ({:2.4f}%) from the OSMNX network\".format(_nodes_removed, _nodes_removed/float(network.number_of_nodes())))\n",
    "    print(\"Number of nodes: {}\".format(network.number_of_nodes()))\n",
    "    print(\"Number of edges: {}\".format(network.number_of_edges()))\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre Process Street network\n",
    "\n",
    "First, check speed limit attribute data and total number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Turn edges into geodataframe\n",
    "edges = ox.graph_to_gdfs(G, nodes=False, edges=True)\n",
    "\n",
    "# Get unique counts of road segments for each speed limit\n",
    "print('Total segments: ', len(edges))\n",
    "print('No speed data: ', edges['maxspeed'].isna().sum())\n",
    "print(edges['maxspeed'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, use the network_setting function to simplify the network and process speed limit data into driving time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# G, hospitals, grid_file, pop_data = file_import (population_dropdown.value, place_dropdown.value)\n",
    "G = network_setting(G)\n",
    "# Create point geometries for each node in the graph, to make constructing catchment area polygons easier\n",
    "for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])\n",
    "# Modify code to react to processor dropdown (got rid of file_import function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, review speed limit attribute data and total number of edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Turn edges into geodataframe\n",
    "edges = ox.graph_to_gdfs(G, nodes=False, edges=True)\n",
    "\n",
    "# Get unique counts of road segments for each speed limit\n",
    "print('Total segments: ', len(edges))\n",
    "print('No speed data: ', edges['maxspeed_fix'].isna().sum())\n",
    "print('Speed\\tn')\n",
    "print(edges['maxspeed_fix'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hospital_setting\n",
    "\n",
    "Finds the nearest OSMNX node for each hospital.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospital: GeoDataFrame of hospitals\n",
    "* G: OSMNX network\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of hospitals with info on nearest OSMNX node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hospital_setting(hospitals, G):\n",
    "    # Create an empty column \n",
    "    hospitals['nearest_osm']=None\n",
    "    # Append the neaerest osm column with each hospitals neaerest osm node\n",
    "    for i in tqdm(hospitals.index, desc=\"Find the nearest osm from hospitals\", position=0):\n",
    "        hospitals['nearest_osm'][i] = ox.get_nearest_node(G, [hospitals['Y'][i], hospitals['X'][i]], method='euclidean') # find the nearest node from hospital location\n",
    "    print ('hospital setting is done')\n",
    "    return(hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Hospitals to Street Network\n",
    "\n",
    "Use the hospital_setting function to attach hospitals to the nearest street network node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hospitals = hospital_setting(hospitals, G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pop_centroid\n",
    "\n",
    "Converts polygon data to centroid points. \n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_data: a GeodataFrame\n",
    "* pop_type: a string, either \"pop\" for general population or \"covid\" for COVID-19 case data\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of centroids with population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To estimate the centroids of census tract / county\n",
    "def pop_centroid (pop_data, pop_type):\n",
    "    pop_data = pop_data.to_crs({'init': 'epsg:4326'})\n",
    "    # If pop is selected in dropdown, select at risk pop where population is greater than 0\n",
    "    if pop_type ==\"pop\":\n",
    "        pop_data=pop_data[pop_data['OverFifty']>=0] \n",
    "    # If covid is selected in dropdown, select where covid cases are greater than 0\n",
    "    if pop_type ==\"covid\":\n",
    "        pop_data=pop_data[pop_data['cases']>=0]\n",
    "    pop_cent = pop_data.centroid # it make the polygon to the point without any other information\n",
    "    # Convert to gdf\n",
    "    pop_centroid = gpd.GeoDataFrame()\n",
    "    i = 0\n",
    "    message = 'Calculating centroids for ' + pop_type\n",
    "    for point in tqdm(pop_cent, desc=message, position=0):\n",
    "        if pop_type== \"pop\":\n",
    "            pop = pop_data.iloc[i]['OverFifty']\n",
    "            code = pop_data.iloc[i]['GEOID']\n",
    "        if pop_type ==\"covid\":\n",
    "            pop = pop_data.iloc[i]['cases']\n",
    "            code = pop_data.iloc[i].ZCTA5CE10\n",
    "        pop_centroid = pop_centroid.append({'code':code,'pop': pop,'geometry': point}, ignore_index=True)\n",
    "        i = i+1\n",
    "    return(pop_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform Population Data into Points\n",
    "\n",
    "Use the pop_centroid function to:\n",
    "1. convert at risk population data from Census Tract polygons into centroid points, and \n",
    "2. convert COVID case data from zip code tabulation area polygons into centroid points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pop_atrisk = pop_centroid(atrisk_data, \"pop\")\n",
    "pop_covid = pop_centroid(covid_data, \"covid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The analysis section implements the enhanced two-step floating catchment area method in three phases. First, functions are defined to manage the computationally intensive analysis in a parallel processing framework. Second, the user may choose between two different populations (at-risk over 50 years, or COVID cases) and two different resource types (ICU beds or ventilators). Third, the functions and user selections are applied to to implement the E2SFCA method.\n",
    "\n",
    "### calculate_catchment_area\n",
    "\n",
    "Calculates a catchment area of things within some distance of a point using a given metric.\n",
    "\n",
    "Function first creates an ego-centric subgraph on the NetworkX road network starting with the nearest OSM node for the hospital and going out to a given distance as measured by the distance unit. We then calculate the convex hull around the nodes in the ego-centric subgraph and make it a GeoPandas object.\n",
    "\n",
    "Args:\n",
    "\n",
    "* G: OSMNX network\n",
    "* nearest_osm: OSMNX road network node that is closest to the place of interest (hospital)\n",
    "* distance: the max distance to include in the catchment area\n",
    "* distance_unit: how we measure distance (used by ego_graph), we always use time\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame the catchment area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_catchment_area(G, nearest_osm, distance, distance_unit = \"time\"):\n",
    "    # Consutrct an ego graph based on distance unit for an input node\n",
    "    road_network = nx.ego_graph(G, nearest_osm, distance, distance=distance_unit) \n",
    "    # Create point geometries for all nodes in ego graph\n",
    "    nodes = [Point((data['x'], data['y'])) for node, data in road_network.nodes(data=True)]\n",
    "    # Create a single part geometry of all nodes\n",
    "    polygon = gpd.GeoSeries(nodes).unary_union.convex_hull ## to create convex hull\n",
    "    polygon = gpd.GeoDataFrame(gpd.GeoSeries(polygon)) ## change polygon to geopandas\n",
    "    polygon = polygon.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    return polygon.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hospital_measure_acc\n",
    "\n",
    "Measures the effect of a single hospital on the surrounding area. (Uses `calculate_catchment_area` or `djikstra_cca`)\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_thread\\_id: int used to keep track of which thread this is\n",
    "* hospital: Geopandas dataframe with information on a hospital\n",
    "* pop_data: Geopandas dataframe with population data\n",
    "* distances: Distances in time to calculate accessibility for\n",
    "* weights: how to weight the different travel distances\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * Int (\\_thread\\_id)\n",
    "    * GeoDataFrame of catchment areas with key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_measure_acc (_thread_id, hospital, pop_data, distances, weights):\n",
    "    # Apply catchment calculation for each distance (10, 20, and 30 min)\n",
    "    polygons = []\n",
    "    for distance in distances:\n",
    "        # Append djikstra catchment calculation (uncomment to use)\n",
    "        polygons.append(calculate_catchment_area(G, hospital['nearest_osm'],distance))\n",
    "    # Clip the overlapping distance ploygons (create two donuts + hole)\n",
    "    for i in reversed(range(1, len(distances))):\n",
    "        polygons[i] = gpd.overlay(polygons[i], polygons[i-1], how=\"difference\")\n",
    "    \n",
    "    # Calculate accessibility measurements\n",
    "    num_pops = []\n",
    "    for j in pop_data.index:\n",
    "        point = pop_data['geometry'][j]\n",
    "        # Multiply polygons by weights\n",
    "        for k in range(len(polygons)):\n",
    "            if len(polygons[k]) > 0: # To exclude the weirdo (convex hull is not polygon)\n",
    "                if (point.within(polygons[k].iloc[0][\"geometry\"])):\n",
    "                    num_pops.append(pop_data['pop'][j]*weights[k])  \n",
    "    total_pop = sum(num_pops)\n",
    "    for i in range(len(distances)):\n",
    "        polygons[i]['time']=distances[i]\n",
    "        polygons[i]['total_pop']=total_pop\n",
    "        polygons[i]['icu_beds'] = float(hospital['Adult ICU'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i]['vents'] = float(hospital['Total Vent'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i].crs = { 'init' : 'epsg:4326'}\n",
    "        polygons[i] = polygons[i].to_crs({'init':'epsg:32616'})\n",
    "    # print('\\rCatchment for hospital {:4.0f} complete'.format(_thread_id), end=\" \", flush=True)\n",
    "    print('{:.0f}'.format(_thread_id), end=\" \", flush=True)\n",
    "    return(_thread_id, [ polygon.copy(deep=True) for polygon in polygons ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure_acc_par\n",
    "\n",
    "Parallel implementation of accessibility measurement.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* pop_data: Geodataframe containing population data\n",
    "* network: OSMNX street network\n",
    "* distances: list of distances to calculate catchments for\n",
    "* weights: list of floats to apply to different catchments\n",
    "* num\\_proc: number of processors to use.\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe of catchments with accessibility statistics calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_acc_unpacker(args):\n",
    "    return hospital_measure_acc(*args)\n",
    "\n",
    "# Parallel implementation of previous function\n",
    "def measure_acc_par (hospitals, pop_data, network, distances, weights, num_proc = 4):\n",
    "    catchments = []\n",
    "    for distance in distances:\n",
    "        catchments.append(gpd.GeoDataFrame())\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    hospital_list = [ hospitals.iloc[i] for i in range(len(hospitals)) ]\n",
    "    print(\"Calculating\", len(hospital_list), \"hospital catchments...\\ncompleted number:\", end=\" \")\n",
    "    results = pool.map(hospital_acc_unpacker, zip(range(len(hospital_list)), hospital_list, itertools.repeat(pop_data), itertools.repeat(distances), itertools.repeat(weights)))\n",
    "    print(\"\\nFinished calculating hospital catchments.\")\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(distances)):\n",
    "            catchments[j] = catchments[j].append(results[i][j], sort=False)\n",
    "    return catchments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlap_calc\n",
    "\n",
    "Calculates and aggregates accessibility statistics for one catchment on our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_id: thread ID\n",
    "* poly: GeoDataFrame representing a catchment area\n",
    "* grid_file: a GeoDataFrame representing our grids\n",
    "* weight: the weight to applied for a given catchment\n",
    "* service_type: the service we are calculating for: ICU beds or ventilators\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * thread ID\n",
    "    * Counter object (dictionary for numbers) with aggregated stats by grid ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def overlap_calc(_id, poly, grid_file, weight, service_type):\n",
    "    value_dict = Counter()\n",
    "    if type(poly.iloc[0][service_type])!=type(None):           \n",
    "        value = float(poly[service_type])*weight\n",
    "        # Find polygons that overlap hex grids\n",
    "        intersect = gpd.overlay(grid_file, poly, how='intersection')\n",
    "        # Get the intersection's area\n",
    "        intersect['overlapped']= intersect.area\n",
    "        # Divide overlapping area by total area to get percent\n",
    "        intersect['percent'] = intersect['overlapped']/intersect['area']\n",
    "        # Only choose intersecting catchments that make up greater than 50% of hexagon \n",
    "        intersect=intersect[intersect['percent']>=0.5]\n",
    "        # Pull id\n",
    "        intersect_region = intersect['id']\n",
    "        for intersect_id in intersect_region:\n",
    "            try:\n",
    "                value_dict[intersect_id] +=value\n",
    "            except:\n",
    "                value_dict[intersect_id] = value\n",
    "    return(_id, value_dict)\n",
    "\n",
    "def overlap_calc_unpacker(args):\n",
    "    return overlap_calc(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlapping_function\n",
    "\n",
    "Calculates how all catchment areas overlap with and affect the accessibility of each grid in our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* grid_file: GeoDataFrame of our grid\n",
    "* catchments: GeoDataFrame of our catchments\n",
    "* service_type: the kind of care being provided (ICU beds vs. ventilators)\n",
    "* weights: the weight to apply to each service type\n",
    "* num\\_proc: the number of processors\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe - grid\\_file with calculated stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_function (grid_file, catchments, service_type, weights, num_proc = 4):\n",
    "    grid_file[service_type]=0\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    acc_list = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_list.extend([ catchments[i][j:j+1] for j in range(len(catchments[i])) ])\n",
    "    acc_weights = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_weights.extend( [weights[i]]*len(catchments[i]) )\n",
    "    results = pool.map(overlap_calc_unpacker, zip(range(len(acc_list)), acc_list, itertools.repeat(grid_file), acc_weights, itertools.repeat(service_type)))\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    service_values = results[0]\n",
    "    for result in results[1:]:\n",
    "        service_values+=result\n",
    "    for intersect_id, value in service_values.items():\n",
    "        grid_file.loc[grid_file['id']==intersect_id, service_type] += value\n",
    "    return(grid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization\n",
    "\n",
    "Normalizes our result (Geodataframe) for a given resource (res)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (result, res):\n",
    "    result[res]=(result[res]-min(result[res]))/(max(result[res])-min(result[res]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize E2SFCA model parameters\n",
    "\n",
    "Below you can customize the input of the model:\n",
    "\n",
    "* Processor - the number of processors to use\n",
    "* Population - the population to calculate the measure for\n",
    "* Resource - the hospital resource of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "processor_dropdown = ipywidgets.Dropdown( options=[(\"1\", 1), (\"2\", 2), (\"3\", 3), (\"4\", 4)],\n",
    "    value=4, description=\"Processor: \")\n",
    "\n",
    "population_dropdown = ipywidgets.Dropdown( options=[(\"Population at Risk\", \"pop\"), (\"COVID-19 Patients\", \"covid\") ],\n",
    "    value=\"pop\", description=\"Population: \")\n",
    "\n",
    "resource_dropdown = ipywidgets.Dropdown( options=[(\"ICU Beds\", \"icu_beds\"), (\"Ventilators\", \"vents\") ],\n",
    "    value=\"icu_beds\", description=\"Resource: \")\n",
    "\n",
    "display(processor_dropdown,population_dropdown,resource_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant E2SFCA parameters\n",
    "\n",
    "Define the distance bands (minutes of driving time) and weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances=[10, 20, 30] # Distances in travel time\n",
    "weights=[1.0, 0.68, 0.22] # Weights where weights[0] is applied to distances[0]\n",
    "print('distances: ', distances)\n",
    "print('weights: ', weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate hospital catchment areas and service ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if population_dropdown.value == \"pop\":\n",
    "    catchments = measure_acc_par(hospitals, pop_atrisk, G, distances, weights, num_proc=processor_dropdown.value)\n",
    "elif population_dropdown.value == \"covid\":\n",
    "    catchments = measure_acc_par(hospitals, pop_covid, G, distances, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate local accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for j in range(len(catchments)):\n",
    "    catchments[j] = catchments[j][catchments[j][resource_dropdown.value]!=float('inf')]\n",
    "result = overlapping_function(grid_file, catchments, resource_dropdown.value, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[[\"id\",resource_dropdown.value]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = normalization(result, resource_dropdown.value)\n",
    "result[[\"id\",resource_dropdown.value]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save output to geopackage -- will name the layer according the dropdown parameters\n",
    "result[[\"id\",resource_dropdown.value,\"geometry\"]].to_file('data/derived/public/results.gpkg', \n",
    "                layer='{}_{}'.format(population_dropdown.value,resource_dropdown.value), \n",
    "                driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### Histogram of Accessibility\n",
    "Compare to figure 8 in the publication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a histogram for the selected resource (ICU Beds or Ventilators)\n",
    "if hasattr(result, resource_dropdown.value):\n",
    "    result['icu_beds'].plot.hist(bins=10)\n",
    "    plt.axvline(result['icu_beds'].mean(), color='k', linestyle='dashed', linewidth=1)\n",
    "    plt.savefig('./results/figures/reproduction/{}_{}_histogram.png'.format(population_dropdown.value,resource_dropdown.value))\n",
    "else:\n",
    "    print(resource_dropdown.value, \"not calculated yet.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thematic Maps of Accessibility\n",
    "Compare to figure 7 in the publication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unclassified Accessibility Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_map(output_grid, hospitals, resource):\n",
    "    ax=output_grid.plot(column=resource, \n",
    "                        cmap='PuBuGn',\n",
    "                        figsize=(18,12), \n",
    "                        legend=True, \n",
    "                        zorder=1)\n",
    "    # Next two lines set bounds for our x- and y-axes because it looks like there's a weird \n",
    "    # Point at the bottom left of the map that's messing up our frame (Maja)\n",
    "    ax.set_xlim([325000, 370000])\n",
    "    ax.set_ylim([550000, 600000])\n",
    "    hospitals.plot(ax=ax, \n",
    "                   markersize=10, \n",
    "                   zorder=1, \n",
    "                   c='black', \n",
    "                   legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals = hospitals.to_crs({'init': 'epsg:26971'})\n",
    "result = result.to_crs({'init': 'epsg:26971'})\n",
    "output_map(result, hospitals, resource_dropdown.value)\n",
    "# save as image with file name including the resource value, population value, and buffered / not buffered\n",
    "plt.savefig('./results/figures/reproduction/{}_{}_buff{}_continuous.png'.format(population_dropdown.value, resource_dropdown.value, str(buffered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classified Accessibility Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_map_classified(output_grid, hospitals, resource):\n",
    "    ax=output_grid.plot(column=resource, \n",
    "                        scheme='Equal_Interval', \n",
    "                        k=5, \n",
    "                        linewidth=0,\n",
    "                        cmap='Blues', \n",
    "                        figsize=(18,12), \n",
    "                        legend=True, \n",
    "                        label=\"Acc Measure\",\n",
    "                        zorder=1)\n",
    "    # Next two lines set bounds for our x- and y-axes because it looks like there's a weird \n",
    "    # Point at the bottom left of the map that's messing up our frame (Maja)\n",
    "    ax.set_xlim([325000, 370000])\n",
    "    ax.set_ylim([550000, 600000])\n",
    "    hospitals.plot(ax=ax, \n",
    "                   markersize=10, \n",
    "                   zorder=2,\n",
    "                   c='black',\n",
    "                   legend=False,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_map_classified(result, hospitals, resource_dropdown.value)\n",
    "# save as image with file name including the resource value, population value, and buffered / not buffered\n",
    "plt.savefig('./results/figures/reproduction/{}_{}_buff{}_classified.png'.format(population_dropdown.value, resource_dropdown.value, str(buffered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Reproduction with Original Results\n",
    "\n",
    "Load results provided with the original research compendium from shapefile `data/derived/public/Chicago_ACC.shp`. The file contains fields `hospital_i` for \"ICU Beds\" and `hospital_v` for \"Ventilators\". It is not known exactly which version of code was used to create this set of results, although it appears that a more complete road network was used (probably the full state of Illinois) than was provided with the compendium (Chicago only, without a buffer). It is not known whether the population was population at risk (>50 years old) or COVID patients (cases). However, the statistical and geographic distributions of each are extremly similar once the data has been normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import study results to compare\n",
    "# hospital_i assumed to be for ICU and hospital_v assumed to be for ventilator\n",
    "# however it's unknown whether the population is the COVID-19 population or the AT RISK population\n",
    "fp = 'data/derived/public/Chicago_ACC.shp'\n",
    "og_result = gpd.read_file(fp)\n",
    "og_result.set_index(\"id\")\n",
    "og_result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join original results to current results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.set_index(\"id\")\n",
    "result_compare = result.join(og_result[[\"hospital_i\",\"hospital_v\"]])\n",
    "result_compare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Spearman's Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set original_resource variable to the name of original results column matching the modeling choice\n",
    "if resource_dropdown.value == \"icu_beds\":\n",
    "    original_resource = \"hospital_i\"\n",
    "else:\n",
    "    original_resource = \"hospital_v\"\n",
    "\n",
    "# calculate spearman's rho\n",
    "rho = stats.spearmanr(result_compare[[original_resource, resource_dropdown.value]])\n",
    "\n",
    "# format text output\n",
    "correlationmsg = \"Comparing: \\n\" + \"Original resource: \" + resource_dropdown.value + \" and population: unknown\\n\" + \"Reproduction resource: \" + resource_dropdown.value + \" and population: \" + population_dropdown.value\n",
    "correlationmsg += \"\\nStreet network buffered? \" + str(buffered)\n",
    "print(correlationmsg)\n",
    "\"Rho = \" + str(round(rho.correlation,3)) + \", pvalue = \" + str(round(rho.pvalue,3))\n",
    "print(rho_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result_compare[[original_resource]], result_compare[[resource_dropdown.value]], s=1)\n",
    "plt.xlabel(\"Original\", labelpad=5)\n",
    "plt.ylabel(\"Reproduction\", labelpad=5)\n",
    "plt.text(.45, .08, rho_result, fontsize=8)\n",
    "# save plot as image with file name including icu/ventilators and buffered or not\n",
    "plt.savefig(\"./results/figures/reproduction/compare_{}_buffer_{}.png\".format(resource_dropdown.value, str(buffered)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "The reproduction results closely match the orginal results, with a Spearman's *Rho* close to 1 (perfect correlation). We can reject the NULL hypothesis of no correlation between the reproduction results and the original results. However, we also note several challenges and uncertainties. First, the large file sizes and processing times for the full state of Illinois caused challenges in using GitHub as the repository and scaling the reproduction efforts. This challenge was addressed by the original research team's decision to limit the reproduction study materials to the city of Chicago. Unfortunately, that decision led to unanticpated boundary effects and numerous bugs in the code. We have attempted to resolve all of those issues with this reproduction. Second, the original results were not provided with sufficient metadata to know which permutation of the data and code were used to generate them. They cannot be identically reproduced with the provided Chicago road network, they do not appear to identically match the maps in the published paper, and we have failed to identically them with a new buffered road network from OpenStreetMap. The new buffered network produces more similar results, with no discernable difference whether the at risk population (over the age of 50) or the COVID patients (cases) population has been used.\n",
    "\n",
    "The reproduction effort has identified some additional sources of error and uncertainty which may be addressed with future work. *First*, there will always be boundary effects with the E2SFCA method, which can only be alleviated by including service points to a distance equal to the maximum travel time away from the study extent, and network data and population data to a distance equal to twice the maximum travel time away from the study extent. *Second*, there is substaintial missing data in OpenStreetMap speed limits, and the original study has applied speed limits of 35mph to all unknown road segments, even though many are highways and the non-highway urban speed limit in Illinois is 30mph. *Third*, the study has reaggregated population data to hospital catchments using centroid points, and has re-aggregated catchment areas to the final hexagonal grid if there is 50% or more overlap. Both reaggregations would be more accurate and less sensitive to the modifiable areal unit problem if they used area-weighted reaggregation. Finally, there are numerous opportunities to increase the computational efficiency of the code, e.g. by i) reducing redundancies between different versions of the model by processing and storing data before customizing model parameters, ii) reducing by a third the number of network analysis operations, and iii) using GeoPandas spatial indexing to optimize nearest neighbor searching and overlay analysis.\n",
    "\n",
    "This reproduction study was feasible because the authors developed the research in an open science geospatial cyberinfrastructure community, CyberGISX. They published the majority of code required to execute the study, and made access to the computational infrastructure available to other researchers. This enabled the current research team to reproduce the study without building our own cyberinfrastructure, and with minimal or no prior knowledge using Jupyter notebooks or Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
