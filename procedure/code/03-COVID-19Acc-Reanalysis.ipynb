{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title: Reproduction of Spatial Accessibility of COVID-19 Healthcare Resources in Illinois\n",
    "---\n",
    "\n",
    "**Reproduction of**: Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA\n",
    "\n",
    "Original study *by* Kang, J. Y., A. Michels, F. Lyu, Shaohua Wang, N. Agbodo, V. L. Freeman, and Shaowen Wang. 2020. Rapidly measuring spatial accessibility of COVID-19 healthcare resources: a case study of Illinois, USA. International Journal of Health Geographics 19 (1):1â€“17. DOI:[10.1186/s12942-020-00229-x](https://ij-healthgeographics.biomedcentral.com/articles/10.1186/s12942-020-00229-x).\n",
    "\n",
    "Reproduction Authors: Joe Holler, Derrick Burt, Kufre Udoh, with contributions from Peter Kedron, Drew An-Pham, William Procter, and the Spring 2021/Fall 2023 Open Source GIScience classs at Middlebury\n",
    "\n",
    "Reproduction Materials Available at: [github.com/HEGSRR/RPr-Kang-2020](https://github.com/HEGSRR/RPr-Kang-2020)\n",
    "\n",
    "Created: `2021-06-01`\n",
    "Revised: `2023-12-4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Study Design\n",
    "\n",
    "The original Kang et al.(2020) investigates access to COVID-19 healthcare resources in Chicago using a network analysis and service ratio approach.  They combine and analyze data on the the point locations of hospitals, hospital capacity (specifically ICU beds and ventilators), a road network of Chicago, COVID-19 cases, and census population data on at-risk individuals (over age 50).  Specifically, they generate hospital catchment areas in the vicinity of each hospital using a road network and estimated travel time to each hospital; these are effectively service polygons for each hospital, which are each one is for a specific driving distance from each hospital.   Centroid points are determined for population and COVID-19 case data (for each zip code).  A spatial join (joining the population point data to the each service polygon) is then used to calculate a weighted service ratio for each hospital catchment area.  This data is then unioned to a \"raster\" grid of hexagons, to produce a continuous map of service ratios in Chicago...which is effectively an accessibility measure.  This accessibility measure is weighted on a [0,1] scale, where 1 is the greatest accessibility to critical COVID-19 healthcare resources (ICU beds or ventilators).  The output maps of this study compare spatial accessibility measures for COVID-19 patients to those of population at risk, and identifes which geographic areas need additional healthcare resources to improve access. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproduction Study Design\n",
    "\n",
    "This reproduction seeks to test two amendments to the original methodology.\n",
    "\n",
    "1. Improve speed limit information. The current network analysis methodology uses known speed limits on roads in the network, and employs a simple distance to hospital divided by the speed limit to calculate the travel distance to the nearest hospital, which informs the spatial extent of the service area polygons for each hospital.  However, for roads that do not have a known speed limit in the data (of which there are many), the original authors arbitrarily assign a speed limit of 35mph, even though this may not be realistic (especially for highways).  Similarly, some roads have a range of speed limits.  Thus, we believe that there is a better methodology to computing travel time.  To do this, plan to replace the network_setting function with use of the osmnx.speed module. Note how this changes the speed limit data checks before & after network_setting. To gauge the effect of making this methodological change, we plan to compare graphs of hospital catchment areas between the two methods, and also create a difference map of the hexagon grid to illustrate the changes that this new methodology has.\n",
    "\n",
    "2. Improve translation from hospital catchments into hexagons by employing Area-Weighted Reaggregation.  Rather than use as simple 50% overlap threshold to determine whether service polygon fragments inform the value of a polygon, we will take a more holistic approach and better weight the role of each catchment area on a given hexagon.  For instance, if 3 catchment areas overlap the same hexagon, but all have different overlap amounts.  To do this, focus on the overlap_calc function. Try commenting every line of this function with its purpose before changing anything. To gauge the impact of this methodological change, again, we plan to compare graphs of hospital catchment areas between the two methods, and also create a difference map of the hexagon grid to illustrate the changes that this new methodology has.  We will also do a correlation analysis of these two methodologies of hexagons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Data\n",
    "To perform the ESFCA method, three types of data are required, as follows: (1) road network, (2) population, and (3) hospital information. The road network can be obtained from the [OpenStreetMap Python Library, called OSMNX](https://github.com/gboeing/osmnx). The population data is available on the [American Community Survey](https://data.census.gov/cedsci/deeplinks?url=https%3A%2F%2Ffactfinder.census.gov%2F&tid=GOVSTIMESERIES.CG00ORG01). Lastly, hospital information is also publically available on the [Homelanad Infrastructure Foundation-Level Data](https://hifld-geoplatform.opendata.arcgis.com/datasets/hospitals?geometry=-94.504%2C40.632%2C-80.980%2C43.486)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules\n",
    "Import necessary libraries to run this model.\n",
    "See `environment.yml` for the library versions used for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import re\n",
    "from shapely.geometry import Point, LineString, Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import folium\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "import IPython\n",
    "import requests\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print('\\n'.join(f'{m.__name__}=={m.__version__}' for m in globals().values() if getattr(m, '__version__', None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Directories\n",
    "\n",
    "Because we have restructured the repository for replication, we need to check our working directory and make necessary adjustments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check working directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use to set work directory properly\n",
    "if os.path.basename(os.getcwd()) == 'code':\n",
    "    os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Visualize Data\n",
    "\n",
    "### Population and COVID-19 Cases Data by County\n",
    "\n",
    "*'Cases' column is coming in as 'Unnamed_0' --> easy to rename but this probably should be reportede to the original authors*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to use the data generated from the pre-processing scripts, use the following code:\n",
    "\n",
    "```py\n",
    "covid_data = gpd.read_file('./data/raw/public/Pre-Processing/covid_pre-processed.shp')\n",
    "atrisk_data = gpd.read_file('./data/raw/public/Pre-Processing/atrisk_pre-processed.shp')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in at risk population data\n",
    "atrisk_data = gpd.read_file('./data/raw/public/PopData/Illinois_Tract.shp')\n",
    "atrisk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in covid case data\n",
    "covid_data = gpd.read_file('./data/raw/public/PopData/Chicago_ZIPCODE.shp')\n",
    "covid_data['cases'] = covid_data['cases']\n",
    "covid_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Hospital Data\n",
    "\n",
    "Note that 999 is treated as a \"NULL\"/\"NA\" so these hospitals are filtered out. This data contains the number of ICU beds and ventilators at each hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in hospital data\n",
    "hospitals = gpd.read_file('./data/raw/public/HospitalData/Chicago_Hospital_Info.shp')\n",
    "hospitals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate and Plot Map of Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot hospital data\n",
    "m = folium.Map(location=[41.85, -87.65], tiles='cartodbpositron', zoom_start=10)\n",
    "for i in range(0, len(hospitals)):\n",
    "    folium.CircleMarker(\n",
    "      location=[hospitals.iloc[i]['Y'], hospitals.iloc[i]['X']],\n",
    "      popup=\"{}{}\\n{}{}\\n{}{}\".format('Hospital Name: ',hospitals.iloc[i]['Hospital'],\n",
    "                                      'ICU Beds: ',hospitals.iloc[i]['Adult ICU'],\n",
    "                                      'Ventilators: ', hospitals.iloc[i]['Total Vent']),\n",
    "      radius=5,\n",
    "      color='blue',\n",
    "      fill=True,\n",
    "      fill_opacity=0.6,\n",
    "      legend_name = 'Hospitals'\n",
    "    ).add_to(m)\n",
    "legend_html =   '''<div style=\"position: fixed; width: 20%; heigh: auto;\n",
    "                            bottom: 10px; left: 10px;\n",
    "                            solid grey; z-index:9999; font-size:14px;\n",
    "                            \">&nbsp; Legend<br>'''\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Plot Hexagon Grids (500-meter resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in and plot grid file for Chicago\n",
    "grid_file = gpd.read_file('./data/raw/public/GridFile/Chicago_Grid.shp')\n",
    "grid_file.plot(figsize=(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Buffered Road Network (15 miles ~ 24140.2 meters)\n",
    "\n",
    "If `Chicago_Network_Buffer.graphml` does not already exist, this cell will query the road network from OpenStreetMap.  \n",
    "\n",
    "Each of the road network code blocks may take a few mintues to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# To create a new graph from OpenStreetMap, delete or rename data/raw/private/Chicago_Network_Buffer.graphml \n",
    "# (if it exists), and set OSM to True \n",
    "OSM = True\n",
    "\n",
    "# if buffered street network is not saved, and OSM is preferred, # generate a new graph from OpenStreetMap and save it\n",
    "if not os.path.exists(\"./data/raw/private/Chicago_Network_Buffer.graphml\") and OSM:\n",
    "    print(\"Loading buffered Chicago road network from OpenStreetMap. Please wait... runtime may exceed 9min...\", flush=True)\n",
    "    G = ox.graph_from_place('Chicago', network_type='drive', buffer_dist=24140.2) \n",
    "    print(\"Saving Chicago road network to raw/private/Chicago_Network_Buffer.graphml. Please wait...\", flush=True)\n",
    "    ox.save_graphml(G, './data/raw/private/Chicago_Network_Buffer.graphml')\n",
    "    print(\"Data saved.\")\n",
    "\n",
    "# otherwise, if buffered street network is not saved, download graph from the OSF project\n",
    "elif not os.path.exists(\"./data/raw/private/Chicago_Network_Buffer.graphml\"):\n",
    "    print(\"Downloading buffered Chicago road network from OSF...\", flush=True)\n",
    "    url = 'https://osf.io/download/z8ery/'\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    print(\"Saving buffered Chicago road network to file...\", flush=True)\n",
    "    open('./data/raw/private/Chicago_Network_Buffer.graphml', 'wb').write(r.content)\n",
    "\n",
    "# if the buffered street network is already saved, load it\n",
    "if os.path.exists(\"./data/raw/private/Chicago_Network_Buffer.graphml\"):\n",
    "    print(\"Loading buffered Chicago road network from raw/private/Chicago_Network_Buffer.graphml. Please wait...\", flush=True)\n",
    "    G = ox.load_graphml('./data/raw/private/Chicago_Network_Buffer.graphml') \n",
    "    print(\"Data loaded.\") \n",
    "else:\n",
    "    print(\"Error: could not load the road network from file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Road Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ox.plot_graph(G, node_size = 1, bgcolor = 'white', node_color = 'black', edge_color = \"#333333\", node_alpha = 0.5, edge_linewidth = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check speed limit values\n",
    "\n",
    "Display all the unique speed limit values and count how many network edges (road segments) have each value.\n",
    "We will compare this to our cleaned network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Turn nodes and edges into geodataframes\n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# Get unique counts of road segments for each speed limit\n",
    "print(edges['maxspeed'].value_counts())\n",
    "print(str(len(edges)) + \" edges in graph\")\n",
    "\n",
    "# can we also visualize highways / roads with higher speed limits to check accuracy?\n",
    "# the code above converts the graph into an edges geodataframe, which could theoretically be filtered\n",
    "# by fast road segments and mapped, e.g. in folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### network_setting function\n",
    "\n",
    "Cleans the OSMNX network to work better with drive-time analysis.\n",
    "\n",
    "First, we remove all nodes with 0 outdegree because any hospital assigned to such a node would be unreachable from everywhere. Next, we remove small (under 10 node) *strongly connected components* to reduce erroneously small ego-centric networks. Lastly, we ensure that the max speed is set and in the correct units before calculating time.\n",
    "\n",
    "Args:\n",
    "\n",
    "* network: OSMNX network for the spatial extent of interest\n",
    "\n",
    "Returns:\n",
    "\n",
    "* OSMNX network: cleaned OSMNX network for the spatial extent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two things about this function:\n",
    "# 1) the work to remove nodes is hardly worth it now that OSMnx cleans graphs by default\n",
    "# the function is now only pruning < 300 nodes\n",
    "# 2) try using the OSMnx speed module for setting speeds, travel times\n",
    "# https://osmnx.readthedocs.io/en/stable/user-reference.html#module-osmnx.speed\n",
    "# just be careful about units of speed and time!\n",
    "# the remainder of this code expects 'time' to be measured in minutes\n",
    "\n",
    "# def network_setting(network):\n",
    "#     _nodes_removed = len([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "#     network.remove_nodes_from([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "#     for component in list(nx.strongly_connected_components(network)):\n",
    "#         if len(component)<10:\n",
    "#             for node in component:\n",
    "#                 _nodes_removed+=1\n",
    "#                 network.remove_node(node)\n",
    "#     for u, v, k, data in tqdm(G.edges(data=True, keys=True),position=0):\n",
    "#         if 'maxspeed' in data.keys():\n",
    "#             speed_type = type(data['maxspeed'])\n",
    "#             if (speed_type==str):\n",
    "#                 # Add in try/except blocks to catch maxspeed formats that don't fit Kang et al's cases\n",
    "#                 try:\n",
    "#                     if len(data['maxspeed'].split(','))==2:\n",
    "#                         data['maxspeed_fix']=float(data['maxspeed'].split(',')[0])                  \n",
    "#                     elif data['maxspeed']=='signals':\n",
    "#                         data['maxspeed_fix']=30.0 # drive speed setting as 35 miles\n",
    "#                     else:\n",
    "#                         data['maxspeed_fix']=float(data['maxspeed'].split()[0])\n",
    "#                 except:\n",
    "#                     data['maxspeed_fix']=30.0 #miles\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     data['maxspeed_fix']=float(data['maxspeed'][0].split()[0])\n",
    "#                 except:\n",
    "#                     data['maxspeed_fix']=30.0 #miles\n",
    "#         else:\n",
    "#             data['maxspeed_fix']=30.0 #miles\n",
    "#         data['maxspeed_meters'] = data['maxspeed_fix']*26.8223 # convert mile per hour to meters per minute\n",
    "#         data['time'] = float(data['length'])/ data['maxspeed_meters'] # meters / meters per minute = minutes\n",
    "#     print(\"Removed {} nodes ({:2.4f}%) from the OSMNX network\".format(_nodes_removed, _nodes_removed/float(network.number_of_nodes())))\n",
    "#     print(\"Number of nodes: {}\".format(network.number_of_nodes()))\n",
    "#     print(\"Number of edges: {}\".format(network.number_of_edges()))    \n",
    "#     return(network)\n",
    "\n",
    "# Imputes edge speed in kilometers per hour to roads that are missing speed limits\n",
    "\n",
    "def network_setting(network):\n",
    "    _nodes_removed = len([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    network.remove_nodes_from([n for (n, deg) in network.out_degree() if deg ==0])\n",
    "    for component in list(nx.strongly_connected_components(network)):\n",
    "        if len(component)<10:\n",
    "            for node in component:\n",
    "                _nodes_removed+=1\n",
    "                network.remove_node(node)\n",
    "    ox.speed.add_edge_speeds(network)\n",
    "    ox.speed.add_edge_travel_times(network)\n",
    "    print(\"Removed {} nodes ({:2.4f}%) from the OSMNX network\".format(_nodes_removed, _nodes_removed/float(network.number_of_nodes())))\n",
    "    print(\"Number of nodes: {}\".format(network.number_of_nodes()))\n",
    "    print(\"Number of edges: {}\".format(network.number_of_edges()))\n",
    "    return(network)\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the Network using network_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# G, hospitals, grid_file, pop_data = file_import (population_dropdown.value)\n",
    "G = network_setting(G)\n",
    "\n",
    "# Create point geometries for each node in the graph, to make constructing catchment area polygons easier\n",
    "for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])\n",
    "# Modify code to react to processor dropdown (got rid of file_import function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-check speed limit values\n",
    "\n",
    "Display all the unique speed limit values and count how many network edges (road segments) have each value.\n",
    "Compare to the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Get unique counts of speed limits for each road network...after imputing\n",
    "# Turn nodes and edges in geodataframes\n",
    "nodes, edges = ox.graph_to_gdfs(G, nodes=True, edges=True)\n",
    "\n",
    "# Convert from kph to mph\n",
    "edges['speed_mph'] = edges['speed_kph']*0.62137119223733\n",
    "# Count\n",
    "print(edges['speed_mph'].value_counts())\n",
    "print(str(len(edges)) + \" edges in graph\")\n",
    "print(edges['travel_time'].value_counts()) #in seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Helper\" Functions\n",
    "\n",
    "The functions below are needed for our analysis later, let's take a look!\n",
    "\n",
    "### hospital_setting\n",
    "\n",
    "Finds the nearest network node for each hospital.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospital: GeoDataFrame of hospitals\n",
    "* G: OSMNX network\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of hospitals with info on nearest network node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_setting(hospitals, G):\n",
    "    # Create an empty column \n",
    "    hospitals['nearest_osm']=None\n",
    "    # Append the neaerest osm column with each hospitals neaerest osm node\n",
    "    for i in tqdm(hospitals.index, desc=\"Find the nearest network node from hospitals\", position=0):\n",
    "        hospitals['nearest_osm'][i] = ox.get_nearest_node(G, [hospitals['Y'][i], hospitals['X'][i]], method='euclidean') # find the nearest node from hospital location\n",
    "    print ('hospital setting is done')\n",
    "    return(hospitals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pop_centroid\n",
    "\n",
    "Converts geodata to centroids\n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_data: a GeodataFrame\n",
    "* pop_type: a string, either \"pop\" for general population or \"covid\" for COVID-19 case data\n",
    "\n",
    "Returns:\n",
    "\n",
    "* GeoDataFrame of centroids with population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_centroid (pop_data, pop_type):\n",
    "    pop_data = pop_data.to_crs({'init': 'epsg:4326'})\n",
    "    # If pop is selected in dropdown, select at risk pop where population is greater than 0\n",
    "    if pop_type ==\"pop\":\n",
    "        pop_data=pop_data[pop_data['OverFifty']>=0]\n",
    "    # If covid is selected in dropdown, select where covid cases are greater than 0\n",
    "    if pop_type ==\"covid\":\n",
    "        pop_data=pop_data[pop_data['cases']>=0]\n",
    "    pop_cent = pop_data.centroid # it make the polygon to the point without any other information\n",
    "    # Convert to gdf\n",
    "    pop_centroid = gpd.GeoDataFrame()\n",
    "    i = 0\n",
    "    for point in tqdm(pop_cent, desc='Pop Centroid File Setting', position=0):\n",
    "        if pop_type== \"pop\":\n",
    "            pop = pop_data.iloc[i]['OverFifty']\n",
    "            code = pop_data.iloc[i]['GEOID']\n",
    "        if pop_type ==\"covid\":\n",
    "            pop = pop_data.iloc[i]['cases']\n",
    "            code = pop_data.iloc[i].ZCTA5CE10\n",
    "        pop_centroid = pop_centroid.append({'code':code,'pop': pop,'geometry': point}, ignore_index=True)\n",
    "        i = i+1\n",
    "    return(pop_centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### djikstra_cca_polygons\n",
    "\n",
    "Function written by Joe Holler + Derrick Burt. It is a more efficient way to calculate distance-weighted catchment areas for each hospital. The algorithm runs quicker than the original one (\"calculate_catchment_area\"). It first creates a dictionary (with a node and its corresponding drive time from the hospital) of all nodes within a 30 minute drive time (using single_cource_dijkstra_path_length function). From here, two more dictionaries are constructed by querying the original one. From this dictionaries, single part convex hulls are created for each drive time interval and appended into a single list (one list with 3 polygon geometries). Within the list, the polygons are differenced from each other to produce three catchment areas.\n",
    "\n",
    "Args:\n",
    "* G: cleaned network graph *with node point geometries attached*\n",
    "* nearest_osm: A unique nearest node ID calculated for a single hospital\n",
    "* distances: 3 distances (in drive time) to calculate catchment areas from\n",
    "* distance_unit: unit to calculate (time)\n",
    "\n",
    "Returns:\n",
    "* A list of 3 diffrenced (not-overlapping) catchment area polygons (10 min poly, 20 min poly, 30 min poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dijkstra_cca_polygons(G, nearest_osm, distances, distance_unit = \"travel_time\"):\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    Before running: must assign point geometries to street nodes\n",
    "    \n",
    "    # create point geometries for the entire graph\n",
    "    for node, data in G.nodes(data=True):\n",
    "    data['geometry']=Point(data['x'], data['y'])\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ## CREATE DICTIONARIES\n",
    "    # create dictionary of nearest nodes\n",
    "    nearest_nodes_30 = nx.single_source_dijkstra_path_length(G, nearest_osm, distances[2], distance_unit) # creating the largest graph from which 10 and 20 minute drive times can be extracted from\n",
    "    \n",
    "    # extract values within 20 and 10 (respectively) minutes drive times\n",
    "    nearest_nodes_20 = dict()\n",
    "    nearest_nodes_10 = dict()\n",
    "    for key, value in nearest_nodes_30.items():\n",
    "        if value <= distances[1]:\n",
    "            nearest_nodes_20[key] = value\n",
    "        if value <= distances[0]:\n",
    "            nearest_nodes_10[key] = value\n",
    "    \n",
    "    ## CREATE POLYGONS FOR 3 DISTANCE CATEGORIES (10 min, 20 min, 30 min)\n",
    "    # 30 MIN\n",
    "    # If the graph already has a geometry attribute with point data,\n",
    "    # this line will create a GeoPandas GeoDataFrame from the nearest_nodes_30 dictionary\n",
    "    points_30 = gpd.GeoDataFrame(gpd.GeoSeries(nx.get_node_attributes(G.subgraph(nearest_nodes_30), 'geometry')))\n",
    "\n",
    "    # This line converts the nearest_nodes_30 dictionary into a Pandas data frame and joins it to points\n",
    "    # left_index=True and right_index=True are options for merge() to join on the index values\n",
    "    points_30 = points_30.merge(pd.Series(nearest_nodes_30).to_frame(), left_index=True, right_index=True)\n",
    "\n",
    "    # Re-name the columns and set the geodataframe geometry to the geometry column\n",
    "    points_30 = points_30.rename(columns={'0_x':'geometry','0_y':'z'}).set_geometry('geometry')\n",
    "\n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_30 = gpd.GeoDataFrame(gpd.GeoSeries(points_30.unary_union.convex_hull))\n",
    "    polygon_30 = polygon_30.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # 20 MIN\n",
    "    # Select nodes less than or equal to 20\n",
    "    points_20 = points_30.query(\"z <= 20\")\n",
    "    \n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_20 = gpd.GeoDataFrame(gpd.GeoSeries(points_20.unary_union.convex_hull))\n",
    "    polygon_20 = polygon_20.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # 10 MIN\n",
    "    # Select nodes less than or equal to 10\n",
    "    points_10 = points_30.query(\"z <= 10\")\n",
    "    \n",
    "    # Create a convex hull polygon from the points\n",
    "    polygon_10 = gpd.GeoDataFrame(gpd.GeoSeries(points_10.unary_union.convex_hull))\n",
    "    polygon_10 = polygon_10.rename(columns={0:'geometry'}).set_geometry('geometry')\n",
    "    \n",
    "    # Create empty list and append polygons\n",
    "    polygons = []\n",
    "    \n",
    "    # Append\n",
    "    polygons.append(polygon_10)\n",
    "    polygons.append(polygon_20)\n",
    "    polygons.append(polygon_30)\n",
    "    \n",
    "    # Clip the overlapping distance ploygons (create two donuts + hole)\n",
    "    for i in reversed(range(1, len(distances))):\n",
    "        polygons[i] = gpd.overlay(polygons[i], polygons[i-1], how=\"difference\")\n",
    "\n",
    "    return polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hospital_measure_acc (adjusted to incorporate dijkstra_cca_polygons)\n",
    "\n",
    "Measures the effect of a single hospital on the surrounding area. (Uses `dijkstra_cca_polygons`)\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_thread\\_id: int used to keep track of which thread this is\n",
    "* hospital: Geopandas dataframe with information on a hospital\n",
    "* pop_data: Geopandas dataframe with population data\n",
    "* distances: Distances in time to calculate accessibility for\n",
    "* weights: how to weight the different travel distances\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * Int (\\_thread\\_id)\n",
    "    * GeoDataFrame of catchment areas with key stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_measure_acc (_thread_id, hospital, pop_data, distances, weights):\n",
    "    # Create polygons\n",
    "    polygons = dijkstra_cca_polygons(G, hospital['nearest_osm'], distances)\n",
    "    \n",
    "    # Calculate accessibility measurements\n",
    "    num_pops = []\n",
    "    for j in pop_data.index:\n",
    "        point = pop_data['geometry'][j]\n",
    "        # Multiply polygons by weights\n",
    "        for k in range(len(polygons)):\n",
    "            if len(polygons[k]) > 0: # To exclude the weirdo (convex hull is not polygon)\n",
    "                if (point.within(polygons[k].iloc[0][\"geometry\"])):\n",
    "                    num_pops.append(pop_data['pop'][j]*weights[k])  \n",
    "    total_pop = sum(num_pops)\n",
    "    for i in range(len(distances)):\n",
    "        polygons[i]['time']=distances[i]\n",
    "        polygons[i]['total_pop']=total_pop\n",
    "        polygons[i]['hospital_icu_beds'] = float(hospital['Adult ICU'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i]['hospital_vents'] = float(hospital['Total Vent'])/polygons[i]['total_pop'] # proportion of # of beds over pops in 10 mins\n",
    "        polygons[i].crs = { 'init' : 'epsg:4326'}\n",
    "        polygons[i] = polygons[i].to_crs({'init':'epsg:32616'})\n",
    "    print('{:.0f}'.format(_thread_id), end=\" \", flush=True)\n",
    "    return(_thread_id, [ polygon.copy(deep=True) for polygon in polygons ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measure_acc_par\n",
    "\n",
    "Parallel implementation of accessibility measurement.\n",
    "\n",
    "Args:\n",
    "\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* pop_data: Geodataframe containing population data\n",
    "* network: OSMNX street network\n",
    "* distances: list of distances to calculate catchments for\n",
    "* weights: list of floats to apply to different catchments\n",
    "* num\\_proc: number of processors to use.\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe of catchments with accessibility statistics calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hospital_acc_unpacker(args):\n",
    "    return hospital_measure_acc(*args)\n",
    "\n",
    "# WHERE THE RESULTS ARE POOLED AND THEN REAGGREGATED\n",
    "def measure_acc_par (hospitals, pop_data, network, distances, weights, num_proc = 4):\n",
    "    catchments = []\n",
    "    for distance in distances:\n",
    "        catchments.append(gpd.GeoDataFrame())\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    hospital_list = [ hospitals.iloc[i] for i in range(len(hospitals)) ]\n",
    "    print(\"Calculating\", len(hospital_list), \"hospital catchments...\\ncompleted number:\", end=\" \")\n",
    "    results = pool.map(hospital_acc_unpacker, zip(range(len(hospital_list)), hospital_list, itertools.repeat(pop_data), itertools.repeat(distances), itertools.repeat(weights)))\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    for i in range(len(results)):\n",
    "        for j in range(len(distances)):\n",
    "            catchments[j] = catchments[j].append(results[i][j], sort=False)\n",
    "    return catchments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlap_calc\n",
    "\n",
    "Calculates and aggregates accessibility statistics for one catchment on our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* \\_id: thread ID\n",
    "* poly: GeoDataFrame representing a catchment area\n",
    "* grid_file: a GeoDataFrame representing our grids\n",
    "* weight: the weight to applied for a given catchment\n",
    "* service_type: the service we are calculating for: ICU beds or ventilators\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Tuple containing:\n",
    "    * thread ID\n",
    "    * Counter object (dictionary for numbers) with aggregated stats by grid ID number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def overlap_calc(_id, poly, grid_file, weight, service_type):\n",
    "    value_dict = Counter()\n",
    "    if type(poly.iloc[0][service_type])!=type(None):           \n",
    "        value = float(poly[service_type])*weight\n",
    "        intersect = gpd.overlay(grid_file, poly, how='intersection')\n",
    "        intersect['overlapped']= intersect.area\n",
    "        intersect['percent'] = intersect['overlapped']/intersect['area']\n",
    "        intersect=intersect[intersect['percent']>=0.5]\n",
    "        intersect_region = intersect['id']\n",
    "        for intersect_id in intersect_region:\n",
    "            try:\n",
    "                value_dict[intersect_id] +=value\n",
    "            except:\n",
    "                value_dict[intersect_id] = value\n",
    "    return(_id, value_dict)\n",
    "\n",
    "def overlap_calc_unpacker(args):\n",
    "    return overlap_calc(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overlapping_function\n",
    "\n",
    "Calculates how all catchment areas overlap with and affect the accessibility of each grid in our grid file.\n",
    "\n",
    "Args:\n",
    "\n",
    "* grid_file: GeoDataFrame of our grid\n",
    "* catchments: GeoDataFrame of our catchments\n",
    "* service_type: the kind of care being provided (ICU beds vs. ventilators)\n",
    "* weights: the weight to apply to each service type\n",
    "* num\\_proc: the number of processors\n",
    "\n",
    "Returns:\n",
    "\n",
    "* Geodataframe - grid\\_file with calculated stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_function (grid_file, catchments, service_type, weights, num_proc = 4):\n",
    "    grid_file[service_type]=0\n",
    "    pool = mp.Pool(processes = num_proc)\n",
    "    acc_list = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_list.extend([ catchments[i][j:j+1] for j in range(len(catchments[i])) ])\n",
    "    acc_weights = []\n",
    "    for i in range(len(catchments)):\n",
    "        acc_weights.extend( [weights[i]]*len(catchments[i]) )\n",
    "    results = pool.map(overlap_calc_unpacker, zip(range(len(acc_list)), acc_list, itertools.repeat(grid_file), acc_weights, itertools.repeat(service_type)))\n",
    "    pool.close()\n",
    "    results.sort()\n",
    "    results = [ r[1] for r in results ]\n",
    "    service_values = results[0]\n",
    "    for result in results[1:]:\n",
    "        service_values+=result\n",
    "    for intersect_id, value in service_values.items():\n",
    "        grid_file.loc[grid_file['id']==intersect_id, service_type] += value\n",
    "    return(grid_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization\n",
    "\n",
    "Normalizes our result (Geodataframe) for a given resource (res)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization (result, res):\n",
    "    result[res]=(result[res]-min(result[res]))/(max(result[res])-min(result[res]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### file_import\n",
    "\n",
    "Imports all files we need to run our code and pulls the Illinois network from OSMNX if it is not present (will take a while). \n",
    "\n",
    "**NOTE:** even if we calculate accessibility for just Chicago, we want to use the Illinois network (or at least we should not use the Chicago network) because using the Chicago network will result in hospitals near but outside of Chicago having an infinite distance (unreachable because roads do not extend past Chicago).\n",
    "\n",
    "Args:\n",
    "\n",
    "* pop_type: population type, either \"pop\" for general population or \"covid\" for COVID-19 cases\n",
    "* region: the region to use for our hospital and grid file (\"Chicago\" or \"Illinois\")\n",
    "\n",
    "Returns:\n",
    "\n",
    "* G: OSMNX network\n",
    "* hospitals: Geodataframe of hospitals\n",
    "* grid_file: Geodataframe of grids\n",
    "* pop_data: Geodataframe of population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_map(output_grid, base_map, hospitals, resource):\n",
    "    ax=output_grid.plot(column=resource, cmap='PuBuGn',figsize=(18,12), legend=True, zorder=1)\n",
    "    # Next two lines set bounds for our x- and y-axes because it looks like there's a weird \n",
    "    # Point at the bottom left of the map that's messing up our frame (Maja)\n",
    "    ax.set_xlim([314000, 370000])\n",
    "    ax.set_ylim([540000, 616000])\n",
    "    base_map.plot(ax=ax, facecolor=\"none\", edgecolor='gray', lw=0.1)\n",
    "    hospitals.plot(ax=ax, markersize=10, zorder=1, c='blue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model\n",
    "\n",
    "Below you can customize the input of the model:\n",
    "\n",
    "* Processor - the number of processors to use\n",
    "* Region - the spatial extent of the measure\n",
    "* Population - the population to calculate the measure for\n",
    "* Resource - the hospital resource of interest\n",
    "* Hospital - all hospitals or subset to check code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "from IPython.display import display\n",
    "\n",
    "processor_dropdown = ipywidgets.Dropdown( options=[(\"1\", 1), (\"2\", 2), (\"3\", 3), (\"4\", 4)],\n",
    "    value = 4, description = \"Processor: \")\n",
    "\n",
    "population_dropdown = ipywidgets.Dropdown( options=[(\"Population at Risk\", \"pop\"), (\"COVID-19 Patients\", \"covid\") ],\n",
    "    value = \"pop\", description = \"Population: \")\n",
    "\n",
    "resource_dropdown = ipywidgets.Dropdown( options=[(\"ICU Beds\", \"hospital_icu_beds\"), (\"Ventilators\", \"hospital_vents\") ],\n",
    "    value = \"hospital_icu_beds\", description = \"Resource: \")\n",
    "\n",
    "hospital_dropdown =  ipywidgets.Dropdown( options=[(\"All hospitals\", \"hospitals\"), (\"Subset\", \"hospital_subset\") ],\n",
    "    value = \"hospitals\", description = \"Hospital:\")\n",
    "\n",
    "display(processor_dropdown,population_dropdown,resource_dropdown,hospital_dropdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process population data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if population_dropdown.value == \"pop\":\n",
    "    pop_data = pop_centroid(atrisk_data, population_dropdown.value)\n",
    "elif population_dropdown.value == \"covid\":\n",
    "    pop_data = pop_centroid(covid_data, population_dropdown.value)\n",
    "distances=[600, 1200, 1800] # Distances in travel time (these are in seconds, so 10min, 20min, 30min)\n",
    "weights=[1.0, 0.68, 0.22] # Weights where weights[0] is applied to distances[0]\n",
    "# Other weighting options representing different distance decays\n",
    "# weights1, weights2, weights3 = [1.0, 0.42, 0.09], [1.0, 0.75, 0.5], [1.0, 0.5, 0.1]\n",
    "# it is surprising how long this function takes just to calculate centroids.\n",
    "# why not do it with the geopandas/pandas functions rather than iterating through every item?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process hospital data\n",
    "If you have already run this code and changed the Hospital selection, rerun the Load Hospital Data block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hospitals according to hospital dropdown\n",
    "if hospital_dropdown.value == \"hospital_subset\":\n",
    "    hospitals = hospital_setting(hospitals[:1], G)\n",
    "else: \n",
    "    hospitals = hospital_setting(hospitals, G)\n",
    "resources = [\"hospital_icu_beds\", \"hospital_vents\"] # resources\n",
    "# this is also slower than it needs to be; if network nodes and hospitals are both\n",
    "# geopandas data frames, it should be possible to do a much faster spatial join rather than iterating through every hospital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize catchment areas for first hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create point geometries for entire graph\n",
    "# what is the pupose of the following two lines? Can this be deleted?\n",
    "# for node, data in G.nodes(data=True):\n",
    "#     data['geometry']=Point(data['x'], data['y'])\n",
    "\n",
    "# which hospital to visualize? \n",
    "fighosp = 4\n",
    "\n",
    "# Create catchment for hospital 0\n",
    "poly = dijkstra_cca_polygons(G, hospitals['nearest_osm'][fighosp], distances)\n",
    "\n",
    "# Reproject polygons\n",
    "for i in range(len(poly)):\n",
    "    poly[i].crs = { 'init' : 'epsg:4326'}\n",
    "    poly[i] = poly[i].to_crs({'init':'epsg:32616'})\n",
    "\n",
    "# Reproject hospitals \n",
    "# Possible to map from the hospitals data rather than creating hospital_subset?\n",
    "hospital_subset = hospitals.iloc[[fighosp]].to_crs(epsg=32616)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "min_10 = poly[0].plot(ax=ax, color=\"royalblue\", label=\"10 min drive\")\n",
    "min_20 = poly[1].plot(ax=ax, color=\"cornflowerblue\", label=\"20 min drive\")\n",
    "min_30 = poly[2].plot(ax=ax, color=\"lightsteelblue\", label=\"30 min drive\")\n",
    "\n",
    "hospital_subset.plot(ax=ax, color=\"red\", legend=True, label = \"hospital\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate hospital catchment areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "catchments = measure_acc_par(hospitals, pop_data, G, distances, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accessibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for j in range(len(catchments)):\n",
    "    catchments[j] = catchments[j][catchments[j][resource_dropdown.value]!=float('inf')]\n",
    "result=overlapping_function(grid_file, catchments, resource_dropdown.value, weights, num_proc=processor_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add weight field to each catchment polygon\n",
    "for i in range(len(weights)):\n",
    "    catchments[i]['weight'] = weights[i]\n",
    "# combine the three sets of catchment polygons into one geodataframe\n",
    "geocatchments = pd.concat([catchments[0], catchments[1], catchments[2]])\n",
    "geocatchments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# set weighted to False for original 50% threshold method\n",
    "# switch to True for area-weighted overlay\n",
    "weighted = True\n",
    "\n",
    "# if the value to be calculated is already in the hegaxon grid, delete it\n",
    "# otherwise, the field name gets a suffix _1 in the overlay step\n",
    "if resource_dropdown.value in list(grid_file.columns.values):\n",
    "    grid_file = grid_file.drop(resource_dropdown.value, axis = 1)\n",
    "    \n",
    "# calculate hexagon 'target' areas\n",
    "grid_file['area'] = grid_file.area\n",
    "    \n",
    "# Intersection overlay of hospital catchments and hexagon grid\n",
    "print(\"Intersecting hospital catchments with hexagon grid...\")\n",
    "fragments = gpd.overlay(grid_file, geocatchments, how='intersection')\n",
    "\n",
    "# Calculate percent coverage of the hexagon by the hospital catchment as\n",
    "# fragment area / target(hexagon) area\n",
    "fragments['percent'] = fragments.area / fragments['area']\n",
    "\n",
    "# if using weighted aggregation... \n",
    "if weighted:\n",
    "    print(\"Calculating area-weighted value...\")\n",
    "    # multiply the service/population ratio by the distance weight and the percent coverage\n",
    "    fragments['value'] = fragments[resource_dropdown.value] * fragments['weight'] * fragments['percent']\n",
    "\n",
    "# if using the 50% coverage rule for unweighted aggregation...\n",
    "else:\n",
    "    print(\"Calculating value for hexagons with >=50% overlap...\")\n",
    "    # filter for only the fragments with > 50% coverage by hospital catchment\n",
    "    fragments = fragments[fragments['percent']>=0.5]\n",
    "    # multiply the service/population ration by the distance weight\n",
    "    fragments['value'] = fragments[resource_dropdown.value] * fragments['weight']\n",
    "\n",
    "# select just the hexagon id and value from the fragments,\n",
    "# group the fragments by the (hexagon) id,\n",
    "# and sum the values\n",
    "print(\"Summarizing results by hexagon id...\")\n",
    "sum_results = fragments[['id', 'value']].groupby(by = ['id']).sum()\n",
    "\n",
    "# join the results to the hexagon grid_file based on hexagon id\n",
    "print(\"Joining results to hexagons...\")\n",
    "result_new = pd.merge(grid_file, sum_results, how=\"left\", on = \"id\")\n",
    "\n",
    "# rename value column name to the resource name\n",
    "result_new = result_new.rename(columns = {'value' : resource_dropdown.value + \"_AWR\"})\n",
    "\n",
    "result_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = normalization (result, resource_dropdown.value)\n",
    "\n",
    "result_new = normalization (result_new, (resource_dropdown.value + \"_AWR\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original result using 50% threshold\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New result using AWR\n",
    "result_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results & Discussion\n",
    "\n",
    "OSMNX assigns speedlimits to roads with unknown speed limits based on\n",
    "\n",
    "Depends on overestimation or underestimation of speed limit\n",
    "\n",
    "This is a threat to spatial heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessibility Map (50% threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hospitals = hospitals.to_crs({'init': 'epsg:26971'})\n",
    "result = result.to_crs({'init': 'epsg:26971'})\n",
    "output_map(result, pop_data, hospitals, resource_dropdown.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessibility Map (AWR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hospitals = hospitals.to_crs({'init': 'epsg:26971'})\n",
    "result_new = result_new.to_crs({'init': 'epsg:26971'})\n",
    "output_map(result_new, pop_data, hospitals, resource_dropdown.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two GeoDataFrames based on their geometry\n",
    "merged = pd.merge(result, result_new, how='inner', left_on='geometry', right_on='geometry')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the difference of 'hospital_icu_beds' between the two variables\n",
    "merged['hospital_icu_beds_diff'] = merged['hospital_icu_beds'] - merged['hospital_icu_beds_AWR']\n",
    "\n",
    "merged\n",
    "\n",
    "# Create a new GeoDataFrame with the desired columns\n",
    "#hexagon_diff = merged[['left_x', 'top_x', 'right_x', 'bottom_x', 'id_x', 'area_x', 'geometry', 'hospital_icu_beds_diff']]\n",
    "\n",
    "# Optionally, you can set the CRS of the new GeoDataFrame\n",
    "#hexagon_diff.crs = {'init': 'epsg:26971'}\n",
    "\n",
    "#hexagon_diff\n",
    "\n",
    "#output_map(merged, pop_data, hospitals, hospital_icu_beds_diff)  #output_grid, base_map, hospitals, resource\n",
    "\n",
    "# Print the new GeoDataFrame\n",
    "print(hexagon_diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classified Accessibility Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "to be written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Luo, W., & Qi, Y. (2009). An enhanced two-step floating catchment area (E2SFCA) method for measuring spatial accessibility to primary care physicians. Health & place, 15(4), 1100-1107."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
